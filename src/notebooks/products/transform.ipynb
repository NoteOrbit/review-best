{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Databricks notebook source\n",
        "\n",
        "from pyspark.sql.functions import col, when\n",
        "from pyspark.spark.utils import AnalysisException\n",
        "\n",
        "# --- Unity Catalog Configuration ---\n",
        "catalog_name = \"main_catalog\"\n",
        "schema_name = \"etl_demo\"\n",
        "input_table_name = \"products_raw\"\n",
        "output_table_name = \"products_transformed\"\n",
        "\n",
        "# Use the catalog and schema\n",
        "spark.sql(f\"USE CATALOG {catalog_name}\")\n",
        "spark.sql(f\"USE SCHEMA {schema_name}\")\n",
        "\n",
        "try:\n",
        "    print(f\"Reading raw products data from {catalog_name}.{schema_name}.{input_table_name}...\")\n",
        "    # Read the raw data from the Delta table\n",
        "    df = spark.read.table(f\"{catalog_name}.{schema_name}.{input_table_name}\")\n",
        "    print(f\"Successfully read {df.count()} rows from {catalog_name}.{schema_name}.{input_table_name}.\")\n",
        "\n",
        "    # --- Data Quality Checks ---\n",
        "    # Drop rows with null values in critical columns\n",
        "    critical_columns = [\"product_id\", \"name\", \"price\"]\n",
        "    initial_count = df.count()\n",
        "    df_cleaned = df.na.drop(subset=critical_columns)\n",
        "    cleaned_count = df_cleaned.count()\n",
        "\n",
        "    if initial_count > cleaned_count:\n",
        "        print(f\"Dropped {initial_count - cleaned_count} rows due to null values in critical columns.\")\n",
        "    else:\n",
        "        print(\"No null values found in critical columns.\")\n",
        "\n",
        "    # Filter out products with non-positive prices\n",
        "    df_cleaned = df_cleaned.filter(col(\"price\") > 0)\n",
        "    filtered_count = df_cleaned.count()\n",
        "    if cleaned_count > filtered_count:\n",
        "        print(f\"Dropped {cleaned_count - filtered_count} rows due to non-positive prices.\")\n",
        "    else:\n",
        "        print(\"No products with non-positive prices found.\")\n",
        "\n",
        "    # --- Transformations ---\n",
        "    print(\"Performing transformations...\")\n",
        "    df_transformed = df_cleaned.withColumn(\"price\", col(\"price\").cast(\"double\")) \
",
        "        .withColumn(\"brand\", when(col(\"name\").contains(\"Awesome\"), \"AwesomeBrand\").otherwise(\"GenericBrand\")) \
",
        "        .select(\"product_id\", \"name\", \"brand\", \"price\", \"created_at\")\n",
        "    print(\"Transformations complete.\")\n",
        "\n",
        "    # --- Write Transformed Data ---\n",
        "    print(f\"Writing transformed products data to {catalog_name}.{schema_name}.{output_table_name}...\")\n",
        "    df_transformed.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.{output_table_name}\")\n",
        "    print(f\"Successfully transformed products data and saved to {catalog_name}.{schema_name}.{output_table_name}.\")\n",
        "\n",
        "except AnalysisException as e:\n",
        "    print(f\"Error reading or writing Delta table: {e}\")\n",
        "    print(\"Please ensure the input table exists and has valid Delta table data.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}